library( car )
library( car )
library( ellipse )
library( ellipse )
library( faraway )
library( leaps )
library(MASS)
library( faraway )
library( GGally)
library( GGally)
library(BAS)
library(rgl)
library(corrplot)
library(corrplot)
# import data
savings = read.table(file='savings.txt', header =T)
# Secondly, we fit the complete linear model (Lab 1) and look at the summary of the estimated coefficients.
g = lm( sr ~ pop15 + pop75 + dpi + ddpi, data = savings )
gs = summary( g )
plot( g$res, ylab = "Residuals", main = "Plot of residuals" )
sort( g$res )
sort( g$res ) [ c( 1, 50 ) ]  ## per vedere il primo e l'ultimo residuo
countries = row.names( savings )
identify( 1:50, g$res, countries )
gs = summary(g)
res_std = g$res/gs$sigma
watchout_ids_rstd = which( abs( res_std ) > 2 )
watchout_rstd = res_std[ watchout_ids_rstd ]
watchout_rstd
sort( g$res ) [ c( 1, 50 ) ]  ## per vedere il primo e l'ultimo residuo
plot( g$fitted.values, res_std, ylab = "Standardized Residuals", main = "Standardized Residuals" )
abline( h = c(-2,2), lty = 2, col = 'orange' )
points( g$fitted.values[watchout_ids_rstd],
res_std[watchout_ids_rstd], col = 'red', pch = 16 )
points( g$fitted.values[watchout_ids_lev],
res_std[watchout_ids_lev], col = 'orange', pch = 16 )
legend('topright', col = c('red','orange'),
c('Standardized Residuals', 'Leverages'), pch = rep( 16, 2 ), bty = 'n' )
watchout_ids_lev = seq_along( lev )[ which( lev > 2 * p/n ) ] ## identify the rows relative to leverage points
#Alternatively, we can compute H manually and then extract its diagonal elements:
#manually
H = X %*% solve( t( X ) %*% X ) %*% t( X )
lev = diag( H )
X = model.matrix( g )
X
lev = hat( X )
lev
# or (sono due modi per estrarre il vettore dei valori leva --> diagonale di H)
lev = hatvalues( g )
lev
#Alternatively, we can compute H manually and then extract its diagonal elements:
#manually
H = X %*% solve( t( X ) %*% X ) %*% t( X )
lev = diag( H )
sum(lev) # verifica: sum_i hat( x )_i = p
p = g$rank # p = 5
n = dim(savings)[1] # n = 50
plot( g$fitted.values, lev, ylab = "Leverages", main = "Plot of Leverages",
pch = 16, col = 'black' )
abline( h = 2 * p/n, lty = 2, col = 'red' )
watchout_points_lev = lev[ which( lev > 2 * p/n  ) ]
watchout_points_lev
watchout_ids_lev = seq_along( lev )[ which( lev > 2 * p/n ) ] ## identify the rows relative to leverage points
watchout_ids_lev
points( g$fitted.values[ watchout_ids_lev ], watchout_points_lev, col = 'red', pch = 16 )
lev [ lev >  2 * 5 / 50 ]
sum( lev [ lev >  2 * 5 / 50 ] )
gl = lm( sr ~ pop15 + pop75 + dpi + ddpi, savings, subset = ( lev < 0.2 ) )
summary( gl )
abs( ( g$coefficients - gl$coefficients ) / g$coefficients )
colors = rep( 'black', nrow( savings ) )
colors[ watchout_ids_lev ] = c('red', 'blue', 'green', 'orange')
pairs( savings[ , c( 'sr', 'pop15', 'pop75', 'dpi', 'ddpi' ) ],
pch = 16, col = colors, cex = 1 + 0.5 * as.numeric( colors != 'black' ))
plot( g$res, ylab = "Residuals", main = "Plot of residuals" )
sort( g$res )
sort( g$res ) [ c( 1, 50 ) ]  ## per vedere il primo e l'ultimo residuo
countries = row.names( savings )
identify( 1:50, g$res, countries )
gs = summary(g)
res_std = g$res/gs$sigma
watchout_ids_rstd = which( abs( res_std ) > 2 )
watchout_rstd = res_std[ watchout_ids_rstd ]
watchout_rstd
plot( g$fitted.values, res_std, ylab = "Standardized Residuals", main = "Standardized Residuals" )
abline( h = c(-2,2), lty = 2, col = 'orange' )
points( g$fitted.values[watchout_ids_rstd],
res_std[watchout_ids_rstd], col = 'red', pch = 16 )
points( g$fitted.values[watchout_ids_lev],
res_std[watchout_ids_lev], col = 'orange', pch = 16 )
legend('topright', col = c('red','orange'),
c('Standardized Residuals', 'Leverages'), pch = rep( 16, 2 ), bty = 'n' )
gs = summary( g )
gs$sigma
# manually
stud = g$residuals / ( gs$sigma * sqrt( 1 - lev ) )
# 'rstandard' gives studentized residuals automatically
stud = rstandard( g )
watchout_ids_stud = which( abs( stud ) > 2 )
watchout_stud = stud[ watchout_ids_stud ]
watchout_stud
plot( g$fitted.values, stud, ylab = "Studentized Residuals", main = "Studentized Residuals", pch = 16 )
points( g$fitted.values[watchout_ids_stud],
stud[watchout_ids_stud], col = 'pink', pch = 16 )
points( g$fitted.values[watchout_ids_lev],
stud[watchout_ids_lev], col = 'orange', pch = 16 )
abline( h = c(-2,2), lty = 2, col = 'orange' )
legend('topright', col = c('pink','orange'),
c('Studentized Residual', 'Leverages'), pch = rep( 16, 3 ), bty = 'n' )
Cdist = cooks.distance( g )
watchout_ids_Cdist = which( Cdist > 4/(n-p) )
watchout_Cdist = Cdist[ watchout_ids_Cdist ]
watchout_Cdist
par( mfrow = c( 1, 3 ) )
plot( g$fitted.values, Cdist, pch = 16, xlab = 'Fitted values',
ylab = 'Cooks Distance', main = 'Cooks Distance' )
points( g$fitted.values[ watchout_ids_Cdist ], Cdist[ watchout_ids_Cdist ],
col = 'green', pch = 16 )
plot( g$fitted.values, stud, pch = 16, xlab = 'Fitted values',
ylab = 'Studentized Residuals', main = 'Studentized Residuals' )
points( g$fitted.values[ watchout_ids_stud ], stud[ watchout_ids_stud ],
col = 'pink', pch = 16 )
plot( g$fitted.values, lev, pch = 16, xlab = 'Fitted values',
ylab = 'Leverages', main = 'Leverages' )
points( g$fitted.values[ watchout_ids_lev ], lev[ watchout_ids_lev ],
col = 'orange', pch = 16 )
par( mfrow = c( 1, 1 ) )
#id_to_keep = (1:n)[ - watchout_ids_Cdist ]
id_to_keep = !( 1:n %in% watchout_ids_Cdist )
gl = lm( sr ~ pop15 + pop75 + dpi + ddpi, savings[ id_to_keep, ]  )
summary( gl )
abs( ( gl$coef - g$coef )/g$coef )
# The influential plot represents the studentized residuals vs leverages,
# and highlights them with a circle which is proportional to Cook's distance.
x11()
x11()
influencePlot( g, id.method = "identify", main = "influential Plot",
sub = "Circle size is proportial to Cook's Distance" )
plot(g, which = 5)
plot(g)
5
6
6
7
8
9
0
clc
plot(g, which = 5)
+#__All together: Influential measures__
#`influential.measures` produces a class "infl" object tabular display showing several
#diagnostics measures (such as h_{ii} and Cook's distance).
#Those cases which are influential with respect to any of these measures are marked with an asterisk.
influence.measures( g )
d <- lm(sr ~ pop75 + dpi + ddpi,savings)$res
m <- lm(pop15 ~ pop75 + dpi + ddpi,savings)$res
x11()
plot(m,d,xlab="pop15 residuals",ylab="Saving residuals", main="Partial Regression")
abline(0,g$coef['pop15'])
#Compare the slope on the plot to the original regression and show the line on the plot.
lm(d ~ m)$coef
g$coef
d <- lm(sr ~ pop75 + dpi + ddpi,savings)$res
m <- lm(pop15 ~ pop75 + dpi + ddpi,savings)$res
x11()
plot(m,d,xlab="pop15 residuals",ylab="Saving residuals", main="Partial Regression")
abline(0,g$coef['pop15'])
#Compare the slope on the plot to the original regression and show the line on the plot.
lm(d ~ m)$coef
g$coef
prplot(g,1) # 1 stands for the position of the independent variable  --> library(faraway)
vif( g )
library(BAS)
data(bodyfat) # help(bodyfat)
# summary(bodyfat)
mod = lm(Weight ~ Abdomen, data = bodyfat)
summary(mod)
mod_res = mod$residuals/summary(mod)$sigma
x11()
plot( mod$fitted, mod_res, xlab = 'Fitted values',  ylab = 'Standarzized residuals'  )
qqnorm( mod$residuals )
qqline( mod$residuals, col = 'blue' )
shapiro.test( mod_res )
#The best lambda that is chosen is the one maximizing the likelihood of the transformed data of being
b = boxcox(Weight ~ Abdomen, data = bodyfat)
names(b)
#y likelihood evaluation
#x lambda evaluated
best_lambda_ind = which.max( b$y )
best_lambda = b$x[ best_lambda_ind ]
best_lambda
shapiro.test( mod_res )
#The best lambda that is chosen is the one maximizing the likelihood of the transformed data of being
b = boxcox(Weight ~ Abdomen, data = bodyfat)
names(b)
#y likelihood evaluation
#x lambda evaluated
best_lambda_ind = which.max( b$y )
best_lambda = b$x[ best_lambda_ind ]
best_lambda
#Finally, we test the new model and we investigate the standardized residuals.
mod1 = lm( (Weight ^ best_lambda - 1)/best_lambda ~ Abdomen, data = bodyfat )
summary(mod1)
mod1_res = mod1$residuals/summary( mod1 )$sigma
plot( mod1$fitted, mod1_res, xlab = 'Fitted values',  ylab = 'Standarzized residuals'  )
qqnorm( mod1_res )
abline( 0, 1, col = 'red' )
shapiro.test( residuals( mod1 ) )
data( state )
statedata = data.frame( state.x77, row.names = state.abb, check.names = T )
head( statedata )
g = lm( Life.Exp ~ ., data = statedata )
summary( g )
# remove Area
g1 = update( g, . ~ . - Area )
summary( g1 )
# remove Illiteracy
g2 = update( g1, . ~ . - Illiteracy )
summary( g2 )
# Remove Income
g3 = update( g2, . ~ . - Income )
summary( g3 )
# remove Population
g4 = update( g3, . ~ . - Population )
summary( g4 )
X = statedata [ , -4 ] #not considering the response variable
cor( X )
corrplot(cor(X), method='number')
corrplot(cor(X), method='color')
heatmap( cor( X ), Rowv = NA, Colv = NA, symm = TRUE, keep.dendro = F)
ggpairs(X)
g = lm( Life.Exp ~ ., data = statedata )
g = lm( Life.Exp ~ ., data = statedata )
AIC( g )
BIC( g )
step( g, direction = "backward" , trace = T)   ## direction = "forward" o "both"
step( g, direction = "backward" , trace = T)   ## direction = "forward" o "both"
g_BIC_back = step( g, direction = "backward", k = log(n), trace = T)
# solo matrice dei predittori senza colonna di 1
x = model.matrix( g ) [ , -1 ]
y = statedata$Life
adjr = leaps( x, y, method = "adjr2" )
names(adjr)
adjr
bestmodel_adjr2_ind = which.max( adjr$adjr2 )
adjr$which[ bestmodel_adjr2_ind, ]
# Displays the best 5 models from a leaps object
maxadjr( adjr, 5 )
# Other possibilities
R2 = leaps( x, y, method = "r2" )
bestmodel_R2_ind = which.max( R2$r2 )
R2$which[ bestmodel_R2_ind, ]
data( state )
graphics.off()
clear.all
rm(list=ls())
data( state )
statedata = data.frame( state.x77, row.names = state.abb, check.names = T )
head( statedata )
g = lm( Life.Exp ~ ., data = statedata )
summary( g )
# remove Area
g1 = update( g, . ~ . - Area )
summary( g1 )
# remove Illiteracy
g2 = update( g1, . ~ . - Illiteracy )
summary( g2 )
# Remove Income
g3 = update( g2, . ~ . - Income )
summary( g3 )
# remove Population
g4 = update( g3, . ~ . - Population )
summary( g4 )
AIC(g3, g4)
X = statedata [ , -4 ] #not considering the response variable
cor( X )
corrplot(cor(X), method='number')
corrplot(cor(X), method='color')
heatmap( cor( X ), Rowv = NA, Colv = NA, symm = TRUE, keep.dendro = F)
ggpairs(X)
g = lm( Life.Exp ~ ., data = statedata )
AIC( g )
BIC( g )
step( g, direction = "backward" , trace = T)   ## direction = "forward" o "both"
g_BIC_back = step( g, direction = "backward", k = log(n), trace = T)
g_BIC_back = step( g, direction = "backward", k = log(50), trace = T)
# solo matrice dei predittori senza colonna di 1
x = model.matrix( g ) [ , -1 ]
y = statedata$Life
adjr = leaps( x, y, method = "adjr2" )
names(adjr)
adjr
adjr = leaps( x, y, method = "adjr2" )
names(adjr)
adjr
bestmodel_adjr2_ind = which.max( adjr$adjr2 )
adjr$which[ bestmodel_adjr2_ind, ]
# Displays the best 5 models from a leaps object
maxadjr( adjr, 5 )
# Other possibilities
R2 = leaps( x, y, method = "r2" )
bestmodel_R2_ind = which.max( R2$r2 )
R2$which[ bestmodel_R2_ind, ]
# Other possibilities
R2 = leaps( x, y, method = "r2" )
bestmodel_R2_ind = which.max( R2$r2 )
R2$which[ bestmodel_R2_ind, ]
# Displays the best 5 models from a leaps object
maxadjr( adjr, 5 )
peso   = c( 60, 65, 72, 74, 77, 81, 85, 90 )
altezza = c( 160, 162, 180, 175, 186, 172, 177, 184 )
mod = lm( peso ~ altezza )
summary( mod )
#Compute the CI of predictions.
grid = seq( min( altezza ), max( altezza ), 2 )
# automatic prediction
y.pred = predict( mod, data.frame( altezza = grid ), interval = "confidence", se = T )
names( y.pred )
y.pred$fit
# manually
ndata = cbind( rep( 1, length( grid ) ), grid )
y.pred_fit = ndata %*% mod$coefficients
y.pred_fit
#standard error
y.pred$se
#manually
y.pred_se = rep( 0, 14 )
X = model.matrix( mod )
for( i in 1:14 )
{
y.pred_se[ i ] = summary( mod )$sigma * sqrt( t( ndata[i,] ) %*% solve( t(X) %*% X ) %*% ndata[i,] )
}
y.pred_se
# n - p = 8 - 2 = 6
y.pred$df
tc    = qt( 0.975, length( altezza ) - 2 )
y     = y.pred_fit
y.sup = y.pred_fit + tc * y.pred$se
y.inf = y.pred_fit - tc * y.pred$se
IC = cbind( y, y.inf, y.sup )
IC
y.pred$fit
matplot( grid, cbind( y, y.inf, y.sup ), lty = c( 1, 2, 2 ),
col = c( 1, 'blue', 'blue' ), type = "l", xlab = "altezza",
ylab = "peso", main = 'IC per la media della risposta' )
points( altezza, peso, col = "black", pch = 16 )
y.pred2 = predict( mod, data.frame( altezza = grid ), interval = "prediction", se = T )
y.pred2$fit
y.pred2$fit[ ,1 ] # predicted values \hat{y}_{new}.
y.pred2$fit[ ,2 ] # LB prediction interval for y_{new}.
y.pred2$fit[ ,3 ] # UB prediction interval for y_{new}.
#manually
ndata = cbind( rep( 1, length( grid ) ), grid )
y.pred_fit = ndata %*% mod$coefficients
y.pred_fit
# standard error
y.pred2$se.fit
#manually
y.pred2_se = rep( 0, 14 )
for( i in 1:14 )
{
y.pred2_se[ i ] = summary( mod )$sigma * sqrt(  1 + t( ndata[i,] ) %*% solve( t(X) %*% X ) %*% ndata[i,] )
}
y.pred2_se
tc    = qt( 0.975, length( altezza ) - 2 )
y     = y.pred_fit
y.sup = y.pred_fit + tc * y.pred2_se
y.inf = y.pred_fit - tc * y.pred2_se
IP = cbind( y, y.inf, y.sup )
IP
y.pred2$fit
matplot( grid, y.pred2$fit, lty = c( 1, 2, 2 ), col = c( 1, 2, 2 ), type = "l",
xlab = "altezza", ylab = "peso", main = 'IP per singole osservazioni' )
points( altezza, peso, col = "blue", pch = 16 )
matplot( grid, y.pred2$fit, lty = c( 1, 2, 2 ), col = c( 1, 2, 2 ), type = "l", xlab = "altezza", ylab = "peso",
main = "IC per la media e IP per singole osservazioni" )
lines( grid, y.pred$fit[ , 2 ] , col = "blue", lty = 2, xlab = "altezza", ylab = "peso" )
lines( grid, y.pred$fit[ , 3 ] , col = "blue", lty = 2, xlab = "altezza", ylab = "peso" )
points( altezza, peso, col = "black", pch = 16 )
#According to theory, the prediction interval is broader that the confidence interval (see the standard errors)
#According to theory, the prediction interval is broader that the confidence interval (see the standard errors)
#and all the points are inside the prediction interval, while only few of them are inside the confidence interval.
#According to theory, the prediction interval is broader that the confidence interval (see the standard errors)
#and all the points are inside the prediction interval, while only few of them are inside the confidence interval.
#According to theory, the prediction interval is broader that the confidence interval (see the standard errors)
#and all the points are inside the prediction interval, while only few of them are inside the confidence interval.
#According to theory, the prediction interval is broader that the confidence interval (see the standard errors)
#and all the points are inside the prediction interval, while only few of them are inside the confidence interval.
#According to theory, the prediction interval is broader that the confidence interval (see the standard errors)
#and all the points are inside the prediction interval, while only few of them are inside the confidence interval.
setwd("~/PoliMi/Statistica/Progetto")
stipendi = read.csv("File Salari.csv", header = TRUE);
load("C:/Users/leoma/OneDrive/Documents/PoliMi/Statistica/Progetto/.RData")
stipendi = read.csv("File Salari.csv", header = TRUE);
stats = read.csv("nba stats.csv", header = TRUE);
stipendi = read.csv("File Salari.csv", header = TRUE);
stats = read.csv("nba stats.csv", header = TRUE);
setwd("~/")
stipendi = read.csv("File Salari.csv", header = TRUE);
stipendi = read.csv("File Salari.csv", header = TRUE);
setwd("C:/Users/leoma/OneDrive/Documents/PNRR")
tosc <- read_delim("Toscana.Progetti.csv",
delim = ";", escape_double = FALSE, col_types = cols(geo_map_lat = col_number(),
geo_map_long = col_number()), trim_ws = TRUE)
####Inizio lavoro####
load('dati toscana puliti')
#Prendo i valori popolazione
data <- read_delim("pop.tosc.csv", delim = ";",
escape_double = FALSE, trim_ws = TRUE)
library(readr)
tosc <- read_delim("Toscana.Progetti.csv",
delim = ";", escape_double = FALSE, col_types = cols(geo_map_lat = col_number(),
geo_map_long = col_number()), trim_ws = TRUE)
#Prendo i valori popolazione
data <- read_delim("pop.tosc.csv", delim = ";",
escape_double = FALSE, trim_ws = TRUE)
data$Territorio <- toupper(data$Territorio)
####
comun <- sort(unique(toscana$comune))
source("C:/Users/leoma/OneDrive/Documents/PNRR/Toscana.R", echo=TRUE)
####
comun <- sort(unique(toscana$comune))
inv_per_com <- rep(0, length(comun))
ab_per_com <- rep(0, length(comun))
for (i in 1:length(comun))
{
row <- which(toscana$comune==comun[i])
for (j in 1:length(row))
{
inv_per_com[i] <- inv_per_com[i] + toscana$importo_progetto[row[j]]
}
}
dati.t <- data.frame(comune = comun, investimento = inv_per_com, popolazione = as.numeric(data$Value))
plot(dati.t$popolazione, dati.t$investimento)
graphics.off()
tosc <- read_delim("Toscana.Progetti.csv",
delim = ";", escape_double = FALSE, col_types = cols(geo_map_lat = col_number(),
geo_map_long = col_number()), trim_ws = TRUE)
tosc <- tosc[, 1:12]
####Inizio lavoro####
load('dati toscana puliti')
#Prendo i valori popolazione
data <- read_delim("pop.tosc.csv", delim = ";",
escape_double = FALSE, trim_ws = TRUE)
data$Territorio <- toupper(data$Territorio)
####
comun <- sort(unique(toscana$comune))
inv_per_com <- rep(0, length(comun))
ab_per_com <- rep(0, length(comun))
for (i in 1:length(comun))
{
row <- which(toscana$comune==comun[i])
for (j in 1:length(row))
{
inv_per_com[i] <- inv_per_com[i] + toscana$importo_progetto[row[j]]
}
}
dati.t <- data.frame(comune = comun, investimento = inv_per_com, popolazione = as.numeric(data$Value))
plot(dati.t$popolazione, dati.t$investimento)
coef <- dati.t$investimento/dati.t$popolazione
View(data.frame(comun, coef))
length(which(coef>coef[which(comun=='Pistoia')]))
length(which(coef>coef[which(comun=='Pistoia')]))/length(comun)
87/273
